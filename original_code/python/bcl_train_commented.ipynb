{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "from scipy.stats import beta, multivariate_normal, poisson\n",
    "from scipy.linalg import inv\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# for matlab code to run in vscode pip install https://www.mathworks.com/help/matlab/matlab_external/install-the-matlab-engine-for-python.html\n",
    "data = scipy.io.loadmat(\"C:/Users/szabo/CEU/IBP codes/chunkLearner_IBP/inputs/inputs_2001_exp1_forpy.mat\")\n",
    "\n",
    "X = data['X_2001_exp1'] # matrix for the N shapes' appearance in the T experimental training scenes, size N x T\n",
    "V = data['V_2001_exp1'] # matrix for the N shapes' 2D position in the T experimental training scenes, size N x T x 2\n",
    "X_test = data['Xtest_2001_exp1']  # matrix for the N shapes' appearance in the T experimental test scenes, size N x T\n",
    "V_test = data['Vtest_2001_exp1']  # matrix for the N shapes' 2D position in the T experimental test scenes, size N x T x 2\n",
    "\n",
    "N, T = X.shape\n",
    "\n",
    "# MODEL parameters\n",
    "# prior parameters for the latent chunk's bias and link matrix\n",
    "bAlpha = 1.0    # Beta prior parameter for latent's bias r_k \n",
    "bBeta = 1.0     # Beta prior parameter for latent's bias r_k\n",
    "alpha = 0.1     # IBP prior parameter for the latent link matrix Z\n",
    "\n",
    "# likelihood parameters for the observed shape's appearance x_nt\n",
    "lda = 0.999     # noisy-or parameter for the llh of observed x_nt Eq.5\n",
    "eps = 0.01      # noisy-or parameter for the llh of observed x_nt Eq.5\n",
    "\n",
    "# prior and llh parameters for position variables\n",
    "sigmaU = 3      # variance of the prior of latent chunk position u_kt Eq.3\n",
    "sigmaC = 0.1    # variance of the prior of relative position of the observed shape and its corresponding latent chunk's center c_nk Eq.4\n",
    "sigmaV = 3      # variance in the llh of observed shape's position v_nt Eq.6\n",
    "phi = 4         # scale of variance in the llh of v_nt, in the shape's position appearance relative to the latent chunk's center Eq.6\n",
    "\n",
    "# IMPLEMENTATION parameters\n",
    "# inner iteration parameters\n",
    "Kmax = 1        # maximum number of new latent chunks per inner iteration in the wood_make_gibbs_z_spatial function\n",
    "wburn = 5       # number of burn-in iterations in the wood_make_gibbs_z_spatial function (when adding new chunks)\n",
    "wsample = 10    # number of sampling iterations in the wood_make_gibbs_z_spatial function (when adding new chunks)\n",
    "\n",
    "# outer iteration parameters\n",
    "stepNo = 10\n",
    "burnIn = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingLength = 12 # number of training scenes used in the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pv_trial(x, v, y, Z, sigmaU, sigmaV, phi):\n",
    "# calc_pv_trial calculates trial-wise auxiliary variables used later in calc_pv_training for considering all T trials \n",
    "#\n",
    "# x: [N x 1] = [12 x 1]           binary observation vector for one trial, each shape\n",
    "# v: [N x 1 x 2] = [12 x 1 x 2]   position of all shapes through one trial\n",
    "# y: [K x 1]                      state mtx for the chunks in one trial\n",
    "# Z: [N x K] = [12 x K]           binary dependency mtx for chunk states across trials\n",
    "# phi                             scale of variance in the llh of v_nt, in the shape's position appearance relative to the latent chunk's center Eq.6  \n",
    "#\n",
    "# returns 4 variables, used in calc_pv_training:\n",
    "#   SigmaCtllhInv   is the inverse covariance in Appendix Eq. 46, calculation is in the box at the end of A.3.3.iii, Sigma_{v^t}^{-1}_nk_ml\n",
    "#   wt              defined in Appendix Eq. 42, used later for mean of multidimentional normal distribution in P(V|S), Appendix Eq.48\n",
    "#   Psit            is the normalizing factor, used in Appendix Eq. 48, defined in box at the end of A.3.3.iv \n",
    "#   zc              is the dependency mtx for the active shapes in the trial only \n",
    "#\n",
    "# called by calc_pv_training \n",
    "\n",
    "    N, K = Z.shape\n",
    "    ZAct = Z * np.tile(y.flatten(), (N, 1)) # ZAct:[N x K]  is the dependency mtx for the active chunks (in the trial) only\n",
    "\n",
    "    phi = phi * ZAct                        # phi: [N x K], 0 if no dependency or chunk state is 0 in the trial,  phi if the shape is dependent and chunk is present in the trial\n",
    "    zc = np.logical_and(phi, x)             # zc:  [N x K], 0 if shape is not present, or no dependency, or chunk state is 0 in the trial; 1 if shape is present, there is dependency, and chunk state is 1 in the trial\n",
    "\n",
    "    xAct = np.where(x == 1)[0] + 1          # xAct: [xActNo x 1], indices of the active shapes in the trial\n",
    "    xActNo = len(xAct)                      # xActNo: scalar, number of active shapes in the trial\n",
    "    A = phi / (1 + np.tile(np.sum(phi, axis=1, keepdims=True), (1,K)))  # A: [N x K], Appendix box A.3.1, the probability of the shape being dependent on the chunk, given the chunk is active\n",
    "\n",
    "    sSq = (sigmaV ** 2 / (1 + np.sum(phi, axis=1)))\n",
    "    ptAct  = np.where(np.sum(ZAct * (np.tile(x.flatten(), (K,1)).T), axis=0))[0] + 1 # to match matlab indexing\n",
    "    ptNo = len(ptAct)\n",
    "\n",
    "    SigmaInv = np.zeros((ptNo, ptNo, len(xAct)))\n",
    "    omegaBU = np.zeros((N, N))\n",
    "    diag_values = np.diag(sSq[xAct-1])\n",
    "    omega = 1 / sigmaV**4 * diag_values\n",
    "\n",
    "    availLinks = []\n",
    "    availLinksRow = []\n",
    "    availLinksCol = []\n",
    "\n",
    "    if np.sum(Z) != 0:\n",
    "        availLinksRow, availLinksCol = np.where(Z) # col, row = 1, 0 in matlab 2, 1\n",
    "        availLinks = np.column_stack((availLinksRow+1, availLinksCol+1)) # to match matlab indexing of Z\n",
    "        availLinkNo = availLinks.shape[0]\n",
    "        SigmaCtllhInv = np.zeros((availLinkNo, availLinkNo))\n",
    "        wt = np.zeros((availLinkNo, 2))\n",
    "    else:\n",
    "        availLinkNo = 0\n",
    "        SigmaCtllhInv = 0\n",
    "        wt = 0\n",
    "\n",
    "    if ptNo > 0:\n",
    "        for xi in range(1, len(xAct) + 1):\n",
    "            row_indices = xAct[xi-1]-1\n",
    "            col_indices = ptAct-1\n",
    "            array = A[np.ix_([row_indices], col_indices)]\n",
    "            array_T = array.T\n",
    "            SigmaInv[:, :, xi-1] = (array * array_T) / sSq[xAct[xi-1]-1]\n",
    "    \n",
    "        SigmaCVInv = np.sum(SigmaInv, axis=2)\n",
    "        SigmaCInv = SigmaCVInv + 1 / (sigmaU**2) * np.eye(ptNo)\n",
    "        SigmaC = np.linalg.inv(SigmaCInv)\n",
    "\n",
    "        if SigmaC.size == 1:\n",
    "            ptAct = ptAct[0]\n",
    "            A_submatrix = A[xAct - 1, ptAct - 1]\n",
    "            A_submatrix = A_submatrix.reshape(-1, 1)\n",
    "            omega = omega - (1 / sigmaV**4) * (A_submatrix @ (SigmaC @ A_submatrix.T))  # appendix box after Eq. 36 defining Omega_nm\n",
    "        else:\n",
    "            A_submatrix = A[xAct - 1, :][:, ptAct - 1]\n",
    "            omega = omega - 1 / sigmaV**4 * (A_submatrix @ SigmaC @ A_submatrix.T)      # appendix box after Eq. 36 defining Omega_nm\n",
    "\n",
    "        omegaBU = np.zeros((N,N))\n",
    "        omegaBU[np.ix_(xAct-1, xAct-1)] = omega\n",
    "\n",
    "        for i in range(1, availLinkNo+1):\n",
    "            for j in range(1, availLinkNo+1):\n",
    "                if x[availLinks[i-1, 0]-1] == 1 and x[availLinks[j-1, 0]-1] == 1:\n",
    "                    SigmaCtllhInv[i-1, j-1] = (omegaBU[availLinks[i-1, 0]-1, availLinks[j-1, 0]-1] * \n",
    "                                               phi[availLinks[i-1, 0]-1, availLinks[i-1, 1]-1] * \n",
    "                                               phi[availLinks[j-1, 0]-1, availLinks[j-1, 1]-1])  # appendix box at the end of A.3.3.iii Sigma_{v^t}^{-1}_nk_ml\n",
    "\n",
    "            vnan = np.nan_to_num(v)\n",
    "            #v array shape in the beginning is (12,144,2) due to matlab slicing, using python's (12,2)\n",
    "            first_term = phi[availLinks[i-1,0]-1, availLinks[i-1,1]-1]\n",
    "            second_term = omegaBU[availLinks[i-1,0]-1,:]\n",
    "            result1 = first_term * second_term\n",
    "            result1 = result1.reshape(1,-1)\n",
    "\n",
    "            third_term = np.sum(phi, axis=1) + 1\n",
    "            fourth_term_wt1 = vnan[:,0]\n",
    "            fourth_term_wt2 = vnan[:,1]\n",
    "\n",
    "            result2_wt1 = third_term * fourth_term_wt1\n",
    "            result2_wt2 = third_term * fourth_term_wt2\n",
    "            final_result_wt1 = np.dot(result1, result2_wt1)\n",
    "            final_result_wt2 = np.dot(result1, result2_wt2)\n",
    "\n",
    "            wt[i-1, 0] = final_result_wt1   # appendix Eq.42\n",
    "            wt[i-1, 1] = final_result_wt2   # appendix Eq.42\n",
    "\n",
    "    else:\n",
    "        SigmaC = 1\n",
    "\n",
    "    if type(SigmaC) == int:\n",
    "        det_SigmaC = 1\n",
    "    else:\n",
    "        det_SigmaC = np.linalg.det(SigmaC)\n",
    "\n",
    "    Psit = np.zeros((1, 1, 2))\n",
    "    vp = (1 + np.sum(phi, axis=1)) * v[:, 0]\n",
    "    vpnan = np.nan_to_num(vp)\n",
    "    Psit[:,:,0] = (2*np.pi)**(-xActNo/2) * sigmaU**(-ptNo) * 1/(np.sqrt(np.prod(sSq[xAct-1]))) * det_SigmaC**(0.5) * np.exp(-0.5 * np.dot(vpnan.T, np.dot(omegaBU, vpnan))) # box at the end of A.3.3.iv\n",
    "    \n",
    "    vp = (1 + np.sum(phi, axis=1)) * v[:, 1]\n",
    "    vpnan = np.nan_to_num(vp)\n",
    "    Psit[:,:,1] = (2*np.pi)**(-xActNo/2) * sigmaU**(-ptNo) * 1/(np.sqrt(np.prod(sSq[xAct-1]))) * det_SigmaC**(0.5) * np.exp(-0.5 * np.dot(vpnan.T, np.dot(omegaBU, vpnan))) # box at the end of A.3.3.iv\n",
    "    \n",
    "    return SigmaCtllhInv, wt, Psit, zc\n",
    "\n",
    "\n",
    "def calc_pv_training(x, v, y, Z, sigmaU, sigmaV, phi, sigmaC, *args):\n",
    "# calc_pv_training calculates likelihood of the observed positions P(V|Z, Y, model params) \n",
    "#   x: [N x T]      observation mtx\n",
    "#   v: [N x T x 2]  position of shapes\n",
    "#   y: [K x T]      state mtx of chunks\n",
    "#   Z: [N x K]      dependency mtx\n",
    "#   sigmaU          variance of the chunk's position \n",
    "#   sigmaV          variance of shape's position\n",
    "#   sigmaC          variance of relative position\n",
    "#   *args           1st: control of the dimensions to be evaluated (1: x only, 2: y only, 0 default: both)\n",
    "#                   2nd: is logharithmic\n",
    "#\n",
    "# returns 4 variables:\n",
    "#   p               p(V|Z, Y, model params), used for the Gibbs sampler in wood_make_gibbs_y_spatial and wood_make_gibbs_z_spatial\n",
    "#   muCTpost        mean of the normal distribution in Appendix Eq. 48\n",
    "#   SigmaCTpostInv  inverse covariance in Appendix Eq. 49\n",
    "#   SigmaCTpost     covariance in Appendix Eq. 49\n",
    "#\n",
    "# dependent on calc_pv_trial\n",
    "# called by wood_make_gibbs_y_spatial and wood_make_gibbs_z_spatial\n",
    "    nargs = len(args)\n",
    "    dims = args[0] if nargs > 0 else 0\n",
    "    logOutput = args[1] if nargs > 1 else 0\n",
    "\n",
    "    N, T = x.shape\n",
    "    K = Z.shape[1]\n",
    "    availLinkNo = int(np.sum(Z))    # availLinkNo counts the dependencies between chunks and shapes; note that here the dimensions are not N or K, but the number of all the dependecies\n",
    "    zc = np.zeros((N, K), dtype=bool)\n",
    "    zl = Z.astype(bool)\n",
    "    SigmaCtllhInv = np.zeros((availLinkNo, availLinkNo, T))\n",
    "    wt = np.zeros((availLinkNo, T, 2))\n",
    "    Psit = np.zeros((1, T, 2))\n",
    "\n",
    "    for t in range(1, T+1):\n",
    "\n",
    "        SigmaCtllhInv[:, :, t-1], wt[:, t-1, :], Psit[:, t-1, :], zct = calc_pv_trial(x[:, t-1:t], v[:, t-1, :] , y[:, t-1:t], Z, sigmaU, sigmaV, phi) # calculating stuff for Appendix Eq. 48\n",
    "        zc = np.bitwise_or(zc, zct)\n",
    "       \n",
    "    SigmaCTllhInv = np.sum(SigmaCtllhInv, axis=2) # Appendix box with Eq. 46 in it, definition of Sigma_{v^T}^{-1}\n",
    "    SigmaCTpostInv = SigmaCTllhInv + np.diag(np.ones(availLinkNo)) / (sigmaC ** 2) # Appendix box with Eq. 49 in it (no eq. reference, will correct)\n",
    "    SigmaCTpost = np.linalg.inv(SigmaCTpostInv)\n",
    "\n",
    "    muCTpost = np.dot(SigmaCTpost, np.sum(wt[:, :, 0], axis=1)) # Appendix Eq. 49\n",
    "    muCTpost = muCTpost[:, np.newaxis]\n",
    "    muCTpost = np.concatenate((muCTpost, np.dot(SigmaCTpost, np.sum(wt[:,:,1], axis=1))[:, np.newaxis]), axis=1) # Appendix Eq. 49\n",
    "   \n",
    "    muCTpostX = muCTpost[:, 0]\n",
    "    muCTpostY = muCTpost[:, 1]\n",
    "\n",
    "    #matches the result in matlab mvnormdfln\n",
    "    if np.sum(zc):\n",
    "        zc_zl = zc[zl]\n",
    "        nfx = multivariate_normal.logpdf(muCTpostX[zc_zl], mean=np.zeros(np.sum(zc)), cov=inv(SigmaCTpostInv[zc_zl, :][:, zc_zl]))  # normal distribution in denominator Appendix Eq.48 \n",
    "        nfy = multivariate_normal.logpdf(muCTpostY[zc_zl], mean=np.zeros(np.sum(zc)), cov=inv(SigmaCTpostInv[zc_zl, :][:, zc_zl]))  # normal distribution in denominator Appendix Eq.48 \n",
    "    else:\n",
    "        nfx = 0\n",
    "        nfy = 0\n",
    "\n",
    "    if logOutput == 0:\n",
    "        p1 = np.prod(Psit[:, :, 0]) * (2 * np.pi) ** (-np.sum(zc) / 2) * sigmaC ** (-np.sum(zc)) / np.exp(nfx)  # Appendix Eq.48\n",
    "        p2 = np.prod(Psit[:, :, 1]) * (2 * np.pi) ** (-np.sum(zc) / 2) * sigmaC ** (-np.sum(zc)) / np.exp(nfy)  # Appendix Eq.48\n",
    "\n",
    "        if dims == 0:\n",
    "            p = p1 * p2\n",
    "        elif dims == 1:\n",
    "            p = p1\n",
    "        elif dims == 2:\n",
    "            p = p2\n",
    "        else:\n",
    "            raise ValueError('calc_pv_training: wrong setting for optional argument dims')\n",
    "    else:\n",
    "        lp1 = np.sum(np.log(Psit[:, :, 0])) + np.log((2 * np.pi) ** (-np.sum(zc) / 2) * sigmaC ** (-np.sum(zc))) - nfx  # Appendix Eq.48\n",
    "        lp2 = np.sum(np.log(Psit[:, :, 1])) + np.log((2 * np.pi) ** (-np.sum(zc) / 2) * sigmaC ** (-np.sum(zc))) - nfy  # Appendix Eq.48\n",
    "\n",
    "        if dims == 0:\n",
    "            p = lp1 + lp2\n",
    "        elif dims == 1:\n",
    "            p = lp1\n",
    "        elif dims == 2:\n",
    "            p = lp2\n",
    "        else:\n",
    "            raise ValueError('calc_pv_training: wrong setting for optional argument dims')\n",
    "\n",
    "    return p, muCTpost, SigmaCTpostInv, SigmaCTpost\n",
    "\n",
    "\n",
    "def calc_px_training(X, Y, Z, lda, eps, *args):\n",
    "# calc_px_training calculates the likelihood of the observed shape's appearance in trial t P(X_t|Z, y_:t, model params)\n",
    "#   X: [N x 1] = [12 x 1]           binary observation mtx for all shapes through the given trial, X = X(:,t)\n",
    "#   Y: [K x 1] = [K x 1]            binary state mtx for all chunks for the given trial, Y = Y(:,t)\n",
    "#   Z: [N x K] = [144 x K]          binary dependency mtx for all shapes and chunks \n",
    "#   lda, eps                        parameters of the noisy-or distribution in Eq. 5\n",
    "# called by wood_make_gibbs_y_spatial and wood_make_gibbs_z_spatial\n",
    "    nargs = len(args)\n",
    "    isLog = args[0] if nargs > 0 else 0\n",
    "\n",
    "    if Z.ndim == 1:  #cases (1,)  (2,)  (3,) (4,)\n",
    "        Z = Z.reshape(-1, 1) \n",
    "        pp = (1 - ((1 - lda) ** (Z.T @ Y)) * (1 - eps)) * X + (((1 - lda) ** (Z.T @ Y)) * (1 - eps)) * (1 - X)\n",
    "    else:  #cases (12,1) (12,3)\n",
    "        pp = (1 - ((1 - lda) ** (Z @ Y)) * (1 - eps)) * X + (((1 - lda) ** (Z @ Y)) * (1 - eps)) * (1 - X)\n",
    "\n",
    "    if isLog == 0:\n",
    "        p = np.prod(pp)\n",
    "    else:\n",
    "        pp = np.log(pp)\n",
    "        p = np.sum(pp)\n",
    "\n",
    "    return p\n",
    "\n",
    "def wood_make_gibbs_y_spatial(Y, X, V, Z, R, lda, eps, sigmaU, sigmaV, phi, sigmaC):\n",
    "# wood_make_gibbs_y_spatial is the Gibbs sampler for the observed shape's appearance Y \n",
    "# given the latent link mtx Z and bias R, the observed X and V and the model parameters\n",
    "# section 2.3.1, Eqs 10-12 and Appendix A.2.1 Eqs 18-19\n",
    "# returns the updated Ynew\n",
    "# dependent on calc_pv_training and calc_px_training\n",
    "# called by wood_ibp_learning_frontend\n",
    "    K, T = Y.shape\n",
    "    Ynew = Y.copy()\n",
    "\n",
    "    lpv_y = np.zeros((1, 2)) # position variable of the latent chunk\n",
    "    lpx_y = np.zeros((1, 2)) # state variable of the latent chunk\n",
    "    \n",
    "    lpv_y_new, _, _, _ = calc_pv_training(X, V, Ynew, Z, sigmaU, sigmaV, phi, sigmaC, 0, 1) \n",
    "    lpx_y_new = calc_px_training(X[:, 0:1], Ynew[:, 0:1], Z, lda, eps, 1) \n",
    "\n",
    "    # update the state for each chunk k, for all trials t\n",
    "    for t in range(1, T + 1):\n",
    "        for k in range(1, K + 1):\n",
    "            lpv_y[0, Ynew[k-1, t-1]] = lpv_y_new\n",
    "            lpx_y[0, Ynew[k-1, t-1]] = lpx_y_new\n",
    "\n",
    "            Ynew[k-1, t-1] = 1 - Ynew[k-1, t-1]\n",
    "\n",
    "            lpv_y[0, Ynew[k-1, t-1]], _, _, _ = calc_pv_training(X, V, Ynew, Z, sigmaU, sigmaV, phi, sigmaC, 0, 1)\n",
    "            lpx_y[0, Ynew[k-1, t-1]] = calc_px_training(X[:, t-1:t], Ynew[:, t-1:t], Z, lda, eps, 1)\n",
    "\n",
    "            p = R[0, k-1] # bias of chunk k\n",
    "            \n",
    "            lPYk1_ZXYmk = np.log(p) + lpx_y[0, 1] + lpv_y[0, 1]     # p(y = 1 | ...) Eq.12\n",
    "            lPYk0_ZXYmk = np.log(1 - p) + lpx_y[0, 0] + lpv_y[0, 0] # p(y = 0 | ...) Eq.12\n",
    "\n",
    "            rt = np.exp(lPYk1_ZXYmk - lPYk0_ZXYmk)\n",
    "\n",
    "            if rt == np.inf:\n",
    "                PYk_ZXYmk = 1\n",
    "            else:\n",
    "                PYk_ZXYmk = rt / (rt + 1) # p1/(p1+p0)\n",
    "\n",
    "            Ynew[k-1, t-1] = PYk_ZXYmk > np.random.rand(1,1) # sampling a Bernoulli(p1/(p1+p0)), Eq. 10-11; using that P(U[0,1]<x)=x\n",
    "\n",
    "            lpv_y_new = lpv_y[0, Ynew[k-1, t-1]]\n",
    "            lpx_y_new = lpx_y[0, Ynew[k-1, t-1]]\n",
    "\n",
    "    return Ynew\n",
    "\n",
    "def wood_make_gibbs_z_spatial(Y, X, V, Z, R, lda, eps, sigmaU, sigmaV, phi, sigmaC, bAlpha, bBeta, alpha, Kmax, wburn, wsample):\n",
    "# wood_make_gibbs_z_spatial is the Gibbs sampler for the Z link mtx\n",
    "# given observed shape's appearance Y mtx, the observed X and V and the model parameters\n",
    "# section 2.3.3, Eqs 14-15 and Appendix A.2.3 Eqs 21-22\n",
    "# returns the updated Znew, Ynew and Rnew\n",
    "# dependent on calc_pv_training and calc_px_training   \n",
    "# called by wood_ibp_learning_frontend\n",
    "    N, K = Z.shape\n",
    "    T = X.shape[1]\n",
    "\n",
    "    Znew = Z.copy()\n",
    "    lpv_y = np.zeros((1,2))\n",
    "    lpx_y = np.zeros((1,2))\n",
    "\n",
    "    wall = wburn + wsample\n",
    "\n",
    "    for n in range(1, N + 1): # update the state for each shape n\n",
    "        Znew = Z.copy()\n",
    "        lpv_y_new, _, _, _ = calc_pv_training(X, V, Y, Znew, sigmaU, sigmaV, phi, sigmaC, 0, 1)\n",
    "        lpx_y_new = calc_px_training(X[n-1, :], Y, Znew[n-1, :], lda, eps, 1)\n",
    "\n",
    "        for k in range(1, K + 1): # update the state for each chunk k\n",
    "            if n - 1 < N - 1:\n",
    "                m_mnk = np.sum(Znew[np.r_[0:n-1, n:N], k-1])\n",
    "            else:\n",
    "                m_mnk = np.sum(Znew[0:n-1, k-1])\n",
    "\n",
    "            th_k = m_mnk / N    # P(z_nk = 1  | Z_{-nk}) in Eq. 14\n",
    "\n",
    "            if m_mnk > 0:\n",
    "                lpv_y[0, int(Znew[n-1, k-1])] = lpv_y_new\n",
    "                lpx_y[0, int(Znew[n-1, k-1])] = lpx_y_new\n",
    "\n",
    "                Znew[n-1, k-1] = 1 - Znew[n-1, k-1]\n",
    "\n",
    "                lpv_y[0, int(Znew[n-1, k-1])], _, _, _ = calc_pv_training(X, V, Y, Znew, sigmaU, sigmaV, phi, sigmaC, 0, 1)\n",
    "                lpx_y[0, int(Znew[n-1, k-1])] = calc_px_training(X[n-1, :], Y, Znew[n-1, :], lda, eps, 1)\n",
    "\n",
    "                lPznk1_XZmnkY = np.log(th_k) + lpx_y[0, 1] + lpv_y[0, 1]        # Eq. 14\n",
    "                lPznk0_XZmnkY = np.log(1 - th_k) + lpx_y[0, 0] + lpv_y[0, 0]    # Eq. 14\n",
    "                \n",
    "                rt = np.exp(lPznk1_XZmnkY - lPznk0_XZmnkY)\n",
    "\n",
    "                if rt == np.inf:\n",
    "                    Pznk_XZmnkY = 1\n",
    "                else:\n",
    "                    Pznk_XZmnkY = rt / (rt + 1)\n",
    "            else:\n",
    "                Pznk_XZmnkY = 0\n",
    "\n",
    "            uni = np.random.rand()\n",
    "\n",
    "            Znew[n-1, k-1] = Pznk_XZmnkY > uni # sampling a Bernoulli(Pznk_XZmnkY); using that P(U[0,1]<x)=x\n",
    "\n",
    "            lpv_y_new = lpv_y[0, int(Znew[n-1, k-1])]\n",
    "            lpx_y_new = lpx_y[0, int(Znew[n-1, k-1])]\n",
    "\n",
    "        Z = Znew.copy()\n",
    "        for k in range(1, K + 1): # update the bias r for each chunk k\n",
    "            R[0, k-1] = beta.rvs((bAlpha + np.sum(Y[k-1, :])), (bBeta + T - np.sum(Y[k-1, :])))\n",
    "\n",
    "        PKnew_XZY = np.zeros((1, Kmax + 1))\n",
    "        PKnew_XZY_an = np.zeros((1, Kmax + 1))\n",
    "        PKnew_XZY_5 = np.zeros((1, Kmax + 1))\n",
    "\n",
    "        Ysample = {}\n",
    "        Rsample = {}\n",
    "\n",
    "        for Knew in range(Kmax + 1): # this whole for loop is for adding Knew new chunks, note that this is inside the shape for loop \n",
    "            lpy_xz = np.zeros((1, wsample))\n",
    "            ZAct = np.hstack((Z, np.zeros((N, Knew))))\n",
    "            if Knew > 0:\n",
    "                ZAct[n - 1, K:K + Knew] = 1\n",
    "            RAdd = beta.rvs(bAlpha, bBeta, size=(1,Knew))\n",
    "            RAct = np.hstack([R, RAdd])\n",
    "            if RAdd.shape == (1,0):\n",
    "                RAdd = 0\n",
    "\n",
    "            # rng result is different here, in matlab [1,0], in python [1,1]\n",
    "            YAct = np.vstack([Y, (np.random.rand(Knew, T) < np.tile(RAdd, (Knew, 1))).astype(int)])\n",
    "\n",
    "            lpv_y_act = np.zeros((1,2))\n",
    "            lpx_y_act = np.zeros((1,2))\n",
    "            \n",
    "            lpv_y_new, _, _, _ = calc_pv_training(X, V, YAct, ZAct, sigmaU, sigmaV, phi, sigmaC, 0, 1)\n",
    "            lpx_y_new = calc_px_training(X[n-1, :], YAct, ZAct[n-1, :], lda, eps, 1)\n",
    "\n",
    "            if Knew > 0:\n",
    "                lpy_v = np.zeros(wsample)\n",
    "                lpy_x = np.zeros(wsample)\n",
    "                lpy_xv = np.zeros(wsample)\n",
    "                for wi in range(1, wall+1):\n",
    "                    for t in range(1, T + 1):\n",
    "                        for k in range(1, Knew + 1):\n",
    "                            lpv_y_act[0, int(YAct[K + k-1, t-1])] = lpv_y_new\n",
    "                            lpx_y_act[0, int(YAct[K + k-1, t-1])] = lpx_y_new\n",
    "\n",
    "                            YAct[K + k-1, t-1] = 1 - YAct[K + k-1, t-1]\n",
    "                            lpv_y_act[0, int(YAct[K + k-1, t-1])], _, _, _ = calc_pv_training(X, V, YAct, ZAct, sigmaU, sigmaV, phi, sigmaC, 0, 1)\n",
    "                            lpx_y_act[0, int(YAct[K + k-1, t-1])] = calc_px_training(X[n-1, :], YAct, ZAct[n-1, :], lda, eps, 1)\n",
    "                            p = RAct[0, (K + k-1)]\n",
    "                            lPYk1_ZXYmk = np.log(p) + lpx_y_act[0,1] + lpv_y_act[0,1]\n",
    "                            lPYk0_ZXYmk = np.log(1 - p) + lpx_y_act[0,0] + lpv_y_act[0,0]\n",
    "\n",
    "                            rt = np.exp(lPYk1_ZXYmk - lPYk0_ZXYmk)\n",
    "\n",
    "                            if rt == np.inf:\n",
    "                                PYk1_ZXYmk = 1\n",
    "                            else:\n",
    "                                PYk1_ZXYmk = rt / (rt + 1)\n",
    "\n",
    "                            uni = np.random.uniform(0,1)\n",
    "\n",
    "                            YAct[K + k-1, t-1] = PYk1_ZXYmk > uni\n",
    "\n",
    "                            lpv_y_new = lpv_y_act[0, int(YAct[K + k-1, t-1])]\n",
    "                            lpx_y_new = lpx_y_act[0, int(YAct[K + k-1, t-1])]\n",
    "\n",
    "                    for k in range(1, Knew + 1): # update the bias r for each new chunk k, Eq. 13\n",
    "                        RAct[0, (K + k-1)] = beta.rvs(bAlpha + np.sum(YAct[K + k-1, :]), bBeta + T - np.sum(YAct[K + k-1, :]))\n",
    "\n",
    "                    if wi > wburn:\n",
    "                        lpy_v[wi - wburn - 1] = lpv_y_new\n",
    "                        lpy_x[wi - wburn - 1] = lpx_y_new\n",
    "                        lpy_xv[wi - wburn - 1] = lpx_y_new + lpv_y_new\n",
    "\n",
    "            else:\n",
    "                lpx_y_new = calc_px_training(X[n-1, :], YAct, ZAct[n-1, :], lda, eps, 1)\n",
    "                lpv_y_new, _, _, _ = calc_pv_training(X, V, YAct, ZAct, sigmaU, sigmaV, phi, sigmaC, 0, 1)\n",
    "                lpy_xv = lpx_y_new + lpv_y_new\n",
    "            # this below is based on some importance sampling, we use the log harmonic mean of the likelihoods (some additional comment needed)\n",
    "            if Knew == 0:\n",
    "                py_xzKnew_5 = lpy_xv\n",
    "            else:\n",
    "                py_xzKnew_5 = np.log(wsample) - np.log(1) + np.max(lpy_xv) - np.log(np.sum(1.0 / np.exp(lpy_xv - np.max(lpy_xv))))\n",
    "\n",
    "            PKnew_XZY_5[0, Knew] = (py_xzKnew_5 + np.log(poisson.pmf(Knew, alpha / N))) # Eq. 15\n",
    "\n",
    "            Ysample[Knew] = YAct\n",
    "            Rsample[Knew] = RAct\n",
    "\n",
    "        lpdf_5 = PKnew_XZY_5 - np.max(PKnew_XZY_5)\n",
    "        pdf_5 = np.exp(lpdf_5) / np.sum(np.exp(lpdf_5))\n",
    "\n",
    "        bar = np.random.rand()\n",
    "        #in MATLAB bar = 0.7792\n",
    "        j = 1\n",
    "        while np.sum(pdf_5.flatten()[:j]) < bar:\n",
    "            j += 1\n",
    "        Knew = j-1 # this is where we decide how many new chunks to add\n",
    "\n",
    "        Znew = np.hstack((Z, np.zeros((N, Knew))))\n",
    "        Znew[n-1, K:K + Knew] = 1\n",
    "\n",
    "        Z = Znew.copy()\n",
    "        Y = Ysample[Knew]\n",
    "        R = Rsample[Knew]\n",
    "\n",
    "        # some housekeeping to remove unused chunks\n",
    "        col_sums = np.sum(Z, axis=0)\n",
    "        nodes2keep = np.nonzero(col_sums)[0]\n",
    "        if nodes2keep.size > 0:\n",
    "            Z = Z[:, nodes2keep]\n",
    "            Y = Y[nodes2keep, :]\n",
    "            R = R[0, nodes2keep].reshape(1,-1)\n",
    "            K = Z.shape[1]\n",
    "        else:\n",
    "            Z = np.zeros((N, 1))\n",
    "            Y = np.zeros((1, T))\n",
    "            R = np.zeros((1, 1))\n",
    "            K = Z.shape[1]\n",
    "\n",
    "    Znew = Z\n",
    "    Ynew = Y\n",
    "    Rnew = R\n",
    "\n",
    "    return Znew, Ynew, Rnew\n",
    "\n",
    "def wood_ibp_learning_frontend(X, V, lda, eps, sigmaU, sigmaV, \n",
    "                               phi, sigmaC, alpha, bAlpha, bBeta, \n",
    "                               Kmax, wburn, wsample, stepNo, burnIn):\n",
    "    \n",
    "    initLatentNo = 1\n",
    "    \n",
    "    # 12 x 144\n",
    "    N, T = X.shape\n",
    "\n",
    "    # 12 x 1\n",
    "    Znew = np.zeros((N, initLatentNo))\n",
    "\n",
    "    # 1 x 1\n",
    "    Rnew = beta.rvs(bAlpha, bBeta, size=(1, initLatentNo))\n",
    "\n",
    "    # 144 x 12\n",
    "    Ynew = np.array((np.random.rand(initLatentNo, T) < np.tile(Rnew.T, (1, T))), dtype=int)\n",
    "\n",
    "    Zpost = np.empty((1, stepNo - burnIn), dtype=object)\n",
    "    Ypost = np.empty((1, stepNo - burnIn), dtype=object)\n",
    "    Rpost = np.empty((1, stepNo - burnIn), dtype=object)\n",
    "    muCTpost = np.empty((1, stepNo - burnIn), dtype=object)\n",
    "    sigmaCTpost = np.empty((1, stepNo - burnIn), dtype=object)\n",
    "\n",
    "    for i in range(1, stepNo+1):\n",
    "        print(\"#\" * 50)\n",
    "        print(f\"iteration: {i-1}\")\n",
    "        print(f\"Znew = \\n{Znew}\\n\")\n",
    "        print(f\"Rnew = \\n{Rnew}\\n\")\n",
    "\n",
    "        Ynew = wood_make_gibbs_y_spatial(Ynew, X, V, Znew, Rnew, lda, eps, sigmaU, sigmaV, phi, sigmaC)\n",
    "        \n",
    "        Znew, Ynew, Rnew = wood_make_gibbs_z_spatial(Ynew, X, V, Znew, Rnew, lda, eps, sigmaU, sigmaV, phi, sigmaC, bAlpha, bBeta, alpha, \n",
    "                                                     Kmax, wburn, wsample)\n",
    "\n",
    "        pi, muCTposti, SigmaCTpostInvi, sigmaCTposti = calc_pv_training(X, V, Ynew, Znew, sigmaU, sigmaV, phi, sigmaC, 0, 1)\n",
    "\n",
    "        if i > burnIn:\n",
    "            Zpost[0, i - burnIn - 1] = Znew\n",
    "            Ypost[0, i - burnIn - 1] = Ynew\n",
    "            Rpost[0, i - burnIn - 1] = Rnew\n",
    "            muCTpost[0, i - burnIn - 1] = muCTposti\n",
    "            sigmaCTpost[0, i - burnIn - 1] = sigmaCTposti\n",
    "\n",
    "    return Zpost, Ypost, Rpost, muCTpost, sigmaCTpost, Kmax, wburn, wsample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "iteration: 0\n",
      "Znew = \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "\n",
      "Rnew = \n",
      "[[0.44912526]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\szabo\\AppData\\Local\\Temp\\ipykernel_7240\\3205860773.py:220: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Ynew[k-1, t-1] = PYk_ZXYmk > np.random.rand(1,1)\n",
      "C:\\Users\\szabo\\AppData\\Local\\Temp\\ipykernel_7240\\3205860773.py:83: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  wt[i-1, 0] = final_result_wt1\n",
      "C:\\Users\\szabo\\AppData\\Local\\Temp\\ipykernel_7240\\3205860773.py:84: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  wt[i-1, 1] = final_result_wt2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "iteration: 1\n",
      "Znew = \n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "Rnew = \n",
      "[[0.97785986 0.32133894 0.23146908 0.2115191  0.40778865 0.33571499\n",
      "  0.20801392]]\n",
      "\n",
      "##################################################\n",
      "iteration: 2\n",
      "Znew = \n",
      "[[1. 0. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "Rnew = \n",
      "[[0.99837694 0.43787739 0.39106807 0.19699801 0.33912158 0.17240954\n",
      "  0.24460704]]\n",
      "\n",
      "##################################################\n",
      "iteration: 3\n",
      "Znew = \n",
      "[[1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      "Rnew = \n",
      "[[0.93242151 0.23643651 0.4822945  0.10202957 0.22133871 0.37159386\n",
      "  0.24099226]]\n",
      "\n",
      "##################################################\n",
      "iteration: 4\n",
      "Znew = \n",
      "[[1. 1. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      "Rnew = \n",
      "[[0.99335506 0.39621274 0.57405395 0.11296111 0.28232959 0.1789616\n",
      "  0.11659827]]\n",
      "\n",
      "##################################################\n",
      "iteration: 5\n",
      "Znew = \n",
      "[[1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      "Rnew = \n",
      "[[0.96665714 0.31613804 0.18713338 0.45169164 0.04778578 0.14421933\n",
      "  0.20690476]]\n",
      "\n",
      "##################################################\n",
      "iteration: 6\n",
      "Znew = \n",
      "[[1. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      "Rnew = \n",
      "[[0.91378147 0.43011037 0.41803626 0.35301987 0.30295417 0.22226389\n",
      "  0.29152153]]\n",
      "\n",
      "##################################################\n",
      "iteration: 7\n",
      "Znew = \n",
      "[[1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      "Rnew = \n",
      "[[0.79391008 0.4146284  0.28360208 0.18830094 0.23950907 0.42078557\n",
      "  0.16601167]]\n",
      "\n",
      "##################################################\n",
      "iteration: 8\n",
      "Znew = \n",
      "[[1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "Rnew = \n",
      "[[0.96246257 0.32661714 0.24432666 0.46965131 0.20691801 0.19609536\n",
      "  0.20044474 0.14473036]]\n",
      "\n",
      "##################################################\n",
      "iteration: 9\n",
      "Znew = \n",
      "[[1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "Rnew = \n",
      "[[0.98138118 0.2224863  0.36733707 0.39581389 0.24430776 0.56351558\n",
      "  0.1604739  0.397159  ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Zpost, Ypost, Rpost, muCTpost, sigmaCTpost, Kmax, wburn, wsample = wood_ibp_learning_frontend(X[:, :trainingLength], V[:, :trainingLength, :], \n",
    "                                                                                                lda, eps, sigmaU, sigmaV, phi, sigmaC,alpha, \n",
    "                                                                                                bAlpha, bBeta, Kmax, wburn, wsample, stepNo, burnIn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
